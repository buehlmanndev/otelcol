receivers:
  filelog:
    include:
      - /var/log/pods/*/*/*.log
    start_at: beginning
    include_file_path: true
    operators:
      # Capture CRI timestamp before stripping envelope
      - id: capture_cri_timestamp
        type: regex_parser
        parse_from: body
        parse_to: attributes
        regex: '^(?P<paas_time_utc>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+Z)'

      # Strip containerd envelope and keep k8s filepath metadata
      - id: container_cri
        type: container
        format: containerd
        add_metadata_from_filepath: true
        
      # Route by structure: JSON vs KV
      - id: log_router
        type: router
        default: multiline_recombine
        routes:
          - expr: 'body matches "^\\s*\\{"'
            output: json_parse

      # JSON route: parse and flatten
      - id: json_parse
        type: json_parser
        parse_from: body
        parse_to: attributes
        output: json_move_message

      - id: json_move_message
        type: move
        from: attributes.message
        to: body
        if: 'attributes.message != nil'
        output: json_flatten_log_level

      - id: json_flatten_log_level
        type: move
        from: attributes.log.level
        to: attributes["log.level"]
        if: 'attributes.log != nil && attributes.log.level != nil'
        output: json_add_operator_tag

      - id: json_add_operator_tag
        type: add
        field: attributes["otel.operator.type"]
        value: JSON
        output: cleanup_logtag

      # Combine multiline plain logs until the next application timestamp/level/structured line. Supports 
      # - default Spring Boot Line format,
      # - Json Format (should be normally directly interpreted as JSON and not end up here),
      # - glog/klog-Format,
      # - Apache Error Log Format,
      # - Apache Access Log Format,
      # - https://github.com/camptocamp/docker-swisstopo-light-basemap format
      - id: multiline_recombine
        type: recombine
        combine_field: body
        combine_with: "\n"
        max_log_size: 10MiB
        source_identifier: attributes["log.file.path"]
        is_first_entry: |
          body matches "^(?:ts=)?\\d{4}-\\d{2}-\\d{2}" or
          body matches "^\\{" or
          body matches "^[IWE]\\d{4}" or
          body matches "^\\[[A-Za-z]{3}\\s+[A-Za-z]{3}\\s+\\d{1,2}\\s+\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?\\s+\\d{4}\\]" or
          body matches "^\\d{1,3}(?:\\.\\d{1,3}){3}\\s+" or
          body matches "^\\[Build_"
        force_flush_period: 1s
        output: parse_plain_level

      # Optionally capture a plain-text log level (non-JSON); keep entry even if it doesn't match
      - id: parse_plain_level
        type: regex_parser
        parse_from: body
        parse_to: attributes
        regex: '^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+\s+-\s+(?P<log_level>[A-Z]+)\s'
        if: 'body matches "^\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}\\.\\d+\\s+-\\s+[A-Z]{1,5}\\s"'
        output: parse_apache_logs

      # Parse log level as sev=ERROR or in Apache Style [Fri Dec 12 14:55:04.520956 2025] [ssl:error] 
      - id: parse_apache_logs
        type: regex_parser
        parse_from: body
        parse_to: attributes
        regex: '(?:\[[^:\]]+:|sev=)(?P<log_level>[A-Za-z]+)'
        if: 'attributes.log_level == nil && (body matches "\\[[^:\\]]+:[A-Za-z]+" or body matches "sev=[A-Za-z]+")'
        output: move_plain_level

      - id: move_plain_level
        type: move
        from: attributes.log_level
        to: attributes["log.level"]
        if: attributes.log_level != nil
        output: multiline_add_operator_tag

      - id: multiline_add_operator_tag
        type: add
        field: attributes["otel.operator.type"]
        value: MULTILINE
        output: cleanup_logtag

      # Minimal cleanup: remove helper fields
      - id: cleanup_logtag
        type: remove
        field: attributes.logtag
        if: 'attributes.logtag != nil'
        output: cleanup_log_iostream

      - id: cleanup_log_iostream
        type: remove
        field: attributes["log.iostream"]
        if: 'attributes["log.iostream"] != nil'
        output: cleanup_message_attribute

      - id: cleanup_message_attribute
        type: remove
        field: attributes.message
        if: 'attributes.message != nil'

processors:
  batch: {}

exporters:
  file:
    path: /output/logs.json
  splunk_hec:
    endpoint: ${SPLUNK_HEC_URL}
    token: ${SPLUNK_HEC_TOKEN}
    disable_compression: true
    source: "otelcol"

service:
  pipelines:
    logs:
      receivers: [filelog]
      processors: [batch]
      exporters: [file, splunk_hec]
